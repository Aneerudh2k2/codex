{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-16 11:06:00 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='khu/paper_analyzer', speculative_config=None, tokenizer='khu/paper_analyzer', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=2725998582, served_model_name=khu/paper_analyzer)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-16 11:06:00 utils.py:660] Found nccl from library /home/aneerudh_m/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "import random\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"khu/paper_analyzer\")\n",
    "\n",
    "llm = LLM(\n",
    "      model=\"khu/paper_analyzer\",\n",
    "      gpu_memory_utilization=0.95,\n",
    "      max_num_batched_tokens=8192 * 4,\n",
    "      block_size=16,\n",
    "      max_num_seqs=250,\n",
    "      seed=random.randint(0, 2**32 - 1),\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "      temperature=0.1,\n",
    "      top_p=0.95,\n",
    "      max_tokens=4096,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = {\n",
    "        \"paper_id\": \"2103.15871\",\n",
    "        \"metadata\": {\n",
    "          \"id\": \"2103.15871\",\n",
    "          \"submitter\": \"Varun Kumar\",\n",
    "          \"authors\": \"Luoxin Chen, Francisco Garcia, Varun Kumar, He Xie, Jianhua Lu\",\n",
    "          \"title\": \"Industry Scale Semi-Supervised Learning for Natural Language\\n  Understanding\",\n",
    "          \"journal_ref\": None,\n",
    "          \"doi\": None,\n",
    "          \"report_no\": None,\n",
    "          \"categories\": \"cs.CL cs.AI cs.LG\",\n",
    "          \"license\": \"http://creativecommons.org/licenses/by/4.0/\",\n",
    "          \"abstract\": \"  This paper presents a production Semi-Supervised Learning (SSL) pipeline\\nbased on the student-teacher framework, which leverages millions of unlabeled\\nexamples to improve Natural Language Understanding (NLU) tasks. We investigate\\ntwo questions related to the use of unlabeled data in production SSL context:\\n1) how to select samples from a huge unlabeled data pool that are beneficial\\nfor SSL training, and 2) how do the selected data affect the performance of\\ndifferent state-of-the-art SSL techniques. We compare four widely used SSL\\ntechniques, Pseudo-Label (PL), Knowledge Distillation (KD), Virtual Adversarial\\nTraining (VAT) and Cross-View Training (CVT) in conjunction with two data\\nselection methods including committee-based selection and submodular\\noptimization based selection. We further examine the benefits and drawbacks of\\nthese techniques when applied to intent classification (IC) and named entity\\nrecognition (NER) tasks, and provide guidelines specifying when each of these\\nmethods might be beneficial to improve large scale NLU systems.\\n\",\n",
    "          \"versions\": [\n",
    "            { \"version\": \"v1\", \"created\": \"Mon, 29 Mar 2021 18:24:02 GMT\" }\n",
    "          ],\n",
    "          \"update_date\": \"2021-03-31\",\n",
    "          \"authors_parsed\": [\n",
    "            [\"Chen\", \"Luoxin\", None],\n",
    "            [\"Garcia\", \"Francisco\", None],\n",
    "            [\"Kumar\", \"Varun\", None],\n",
    "            [\"Xie\", \"He\", None],\n",
    "            [\"Lu\", \"Jianhua\", None]\n",
    "          ]\n",
    "        },\n",
    "        \"discipline\": \"Computer Science\",\n",
    "        \"abstract\": {\n",
    "          \"section\": \"Abstract\",\n",
    "          \"text\": \"  This paper presents a production Semi-Supervised Learning (SSL) pipeline\\nbased on the student-teacher framework, which leverages millions of unlabeled\\nexamples to improve Natural Language Understanding (NLU) tasks. We investigate\\ntwo questions related to the use of unlabeled data in production SSL context:\\n1) how to select samples from a huge unlabeled data pool that are beneficial\\nfor SSL training, and 2) how do the selected data affect the performance of\\ndifferent state-of-the-art SSL techniques. We compare four widely used SSL\\ntechniques, Pseudo-Label (PL), Knowledge Distillation (KD), Virtual Adversarial\\nTraining (VAT) and Cross-View Training (CVT) in conjunction with two data\\nselection methods including committee-based selection and submodular\\noptimization based selection. We further examine the benefits and drawbacks of\\nthese techniques when applied to intent classification (IC) and named entity\\nrecognition (NER) tasks, and provide guidelines specifying when each of these\\nmethods might be beneficial to improve large scale NLU systems.\\n\",\n",
    "          \"cite_spans\": [],\n",
    "          \"ref_spans\": []\n",
    "        },\n",
    "        \"bib_entries\": {\n",
    "          \"d5160b73131a9eab9d63ddd96ab549ca183fc019\": {\n",
    "            \"bib_entry_raw\": \"Gustavo Aguilar, Yuan Ling, Yu Zhang, Benjamin Yao, Xing Fan, and Chenlei Guo. 2020. Knowledge distillation from internal representations. In AAAI, pages 7350–7357.\",\n",
    "            \"contained_arXiv_ids\": [],\n",
    "            \"contained_links\": [],\n",
    "            \"discipline\": \"Computer Science\",\n",
    "            \"ids\": {\n",
    "              \"open_alex_id\": \"https://openalex.org/W2997666887\",\n",
    "              \"arxiv_id\": None,\n",
    "              \"pubmed_id\": None,\n",
    "              \"pmc_id\": None,\n",
    "              \"doi\": \"10.1609/aaai.v34i05.6229\"\n",
    "            }\n",
    "          },\n",
    "          \"d81f85d95d72fdc9f4a428b794712d0570fea6f2\": {\n",
    "            \"bib_entry_raw\": \"Arsenii Ashukha, Alexander Lyzhov, Dmitry Molchanov, and Dmitry Vetrov. 2020. Pitfalls of in-domain uncertainty estimation and ensembling in deep learning.\",\n",
    "            \"contained_arXiv_ids\": [\n",
    "              {\n",
    "                \"id\": \"2002.06470\",\n",
    "                \"text\": \"Pitfalls of in-domain uncertainty estimation and ensembling in deep learning.\",\n",
    "                \"start\": 78,\n",
    "                \"end\": 155\n",
    "              }\n",
    "            ],\n",
    "            \"contained_links\": [],\n",
    "            \"discipline\": \"Computer Science\",\n",
    "            \"ids\": {\n",
    "              \"open_alex_id\": \"https://openalex.org/W2995464762\",\n",
    "              \"arxiv_id\": None,\n",
    "              \"pubmed_id\": None,\n",
    "              \"pmc_id\": None,\n",
    "              \"doi\": None\n",
    "            }\n",
    "          },\n",
    "          \"03e89d2071878c90439d6210f8f5694236144b11\": {\n",
    "            \"bib_entry_raw\": \"David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. 2019. Mixmatch: A holistic approach to semi-supervised learning. In Advances in Neural Information Processing Systems, pages 5049–5059.\",\n",
    "            \"contained_arXiv_ids\": [],\n",
    "            \"contained_links\": [],\n",
    "            \"discipline\": \"\",\n",
    "            \"ids\": {\n",
    "              \"open_alex_id\": \"https://openalex.org/W2978426779\",\n",
    "              \"arxiv_id\": None,\n",
    "              \"pubmed_id\": None,\n",
    "              \"pmc_id\": None,\n",
    "              \"doi\": None\n",
    "            }\n",
    "          },\n",
    "          \"34b790c0eff9787af15534bd9727ba18611a803f\": {\n",
    "            \"bib_entry_raw\": \"Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the eleventh annual conference on Computational learning theory, pages 92–100.\",\n",
    "            \"contained_arXiv_ids\": [],\n",
    "            \"contained_links\": [],\n",
    "            \"discipline\": \"Computer Science\",\n",
    "            \"ids\": {\n",
    "              \"open_alex_id\": \"https://openalex.org/W2048679005\",\n",
    "              \"arxiv_id\": None,\n",
    "              \"pubmed_id\": None,\n",
    "              \"pmc_id\": None,\n",
    "              \"doi\": \"10.1145/279943.279962\"\n",
    "            }\n",
    "          },\n",
    "          \"b0f70c4792d483a662ad260d63c46bc281aff4ae\": {\n",
    "            \"bib_entry_raw\": \"Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2016. Enriching word vectors with subword information.\",\n",
    "            \"contained_arXiv_ids\": [\n",
    "              {\n",
    "                \"id\": \"1607.04606\",\n",
    "                \"text\": \"Enriching word vectors with subword information.\",\n",
    "                \"start\": 73,\n",
    "                \"end\": 121\n",
    "              }\n",
    "            ],\n",
    "            \"contained_links\": [],\n",
    "            \"discipline\": \"Computer Science\",\n",
    "            \"ids\": {\n",
    "              \"open_alex_id\": \"https://openalex.org/W2493916176\",\n",
    "              \"arxiv_id\": None,\n",
    "              \"pubmed_id\": None,\n",
    "              \"pmc_id\": None,\n",
    "              \"doi\": \"10.1162/tacl_a_00051\"\n",
    "            }\n",
    "          },\n",
    "          \"e3f43c42b178886fe6a273c8a515489709acb130\": {\n",
    "            \"bib_entry_raw\": \"Olivier Chapelle, Bernhard Schölkopf, and Alexander Zien. 2006. Introduction to semi-supervised learning. In Semi-Supervised Learning.\",\n",
    "            \"contained_arXiv_ids\": [],\n",
    "            \"contained_links\": [],\n",
    "            \"discipline\": \"Computer Science\",\n",
    "            \"ids\": {\n",
    "              \"open_alex_id\": \"https://openalex.org/W2506128341\",\n",
    "              \"arxiv_id\": None,\n",
    "              \"pubmed_id\": None,\n",
    "              \"pmc_id\": None,\n",
    "              \"doi\": None\n",
    "            }\n",
    "          },\n",
    "          \"98318a040e62c35adf72d55b882beca7b69653b9\": {\n",
    "            \"bib_entry_raw\": \"Luoxin Chen, Weitong Ruan, Xinyue Liu, and Jianhua Lu. 2020. SeqVAT: Virtual adversarial training for semi-supervised sequence labeling. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8801–8811, Online. Association for Computational Linguistics.\",\n",
    "            \"contained_arXiv_ids\": [],\n",
    "            \"contained_links\": [\n",
    "              {\n",
    "                \"url\": \"https://doi.org/10.18653/v1/2020.acl-main.777\",\n",
    "                \"text\": \"SeqVAT: Virtual adversarial training for semi-supervised sequence labeling. In\",\n",
    "                \"start\": 61,\n",
    "                \"end\": 139\n",
    "              }\n",
    "            ],\n",
    "            \"discipline\": None,\n",
    "            \"ids\": None\n",
    "          },\n",
    "          \"0cc0f236f438e0b67d0f42d5c316d96b101f8287\": {\n",
    "            \"bib_entry_raw\": \"Eunah Cho, He Xie, John P. Lalor, Varun Kumar, and William M. Campbell. 2019. Efficient semi-supervised learning for natural language understanding by optimizing diversity. 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU).\",\n",
    "            \"contained_arXiv_ids\": [],\n",
    "            \"contained_links\": [\n",
    "              {\n",
    "                \"url\": \"https://doi.org/10.1109/asru46091.2019.9003747\",\n",
    "                \"text\": \"Efficient semi-supervised learning for natural language understanding by optimizing diversity.\",\n",
    "                \"start\": 78,\n",
    "                \"end\": 172\n",
    "              }\n",
    "            ],\n",
    "            \"discipline\": None,\n",
    "            \"ids\": None\n",
    "          },\n",
    "          \"3e267afacd645acd58cf04cf7a628c50e12a9329\": {\n",
    "            \"bib_entry_raw\": \"Kevin Clark, Minh-Thang Luong, Christopher D. Manning, and Quoc V. Le. 2018. Semi-supervised sequence modeling with cross-view training. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 1914–1925. Association for Computational Linguistics.\",\n",
    "            \"contained_arXiv_ids\": [],\n",
    "            \"contained_links\": [\n",
    "              {\n",
    "                \"url\": \"https://doi.org/10.18653/v1/d18-1217\",\n",
    "                \"text\": \"Semi-supervised sequence modeling with cross-view training. In\",\n",
    "                \"start\": 77,\n",
    "                \"end\": 139\n",
    "              }\n",
    "            ],\n",
    "            \"discipline\": None,\n",
    "            \"ids\": None\n",
    "          },\n",
    "          \"df2545b3a0bf92a4c492db04ec6d7142b197d6eb\": {\n",
    "            \"bib_entry_raw\": \"Alice Coucke, Alaa Saade, Adrien Ball, Théodore Bluche, Alexandre Caulier, David Leroy, Clément Doumouro, Thibault Gisselbrecht, Francesco Caltagirone, Thibaut Lavril, Maël Primet, and Joseph Dureau. 2018. Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces. ArXiv, abs/1805.10190.\",\n",
    "            \"contained_arXiv_ids\": [],\n",
    "            \"contained_links\": [],\n",
    "            \"discipline\": None,\n",
    "            \"ids\": None\n",
    "          },\n",
    "          \"7c58e3eafed3afe2c059d1b766808fcbd350e2bd\": {\n",
    "            \"bib_entry_raw\": \"Yifan Ding, Liqiang Wang, Deliang Fan, and Boqing Gong. 2018. A semi-supervised two-stage approach to learning from noisy labels. 2018 IEEE Winter Conference on Applications of Computer Vision (WACV).\",\n",
    "            \"contained_arXiv_ids\": [],\n",
    "            \"contained_links\": [\n",
    "              {\n",
    "                \"url\": \"https://doi.org/10.1109/wacv.2018.00138\",\n",
    "                \"text\": \"A semi-supervised two-stage approach to learning from noisy labels.\",\n",
    "                \"start\": 62,\n",
    "                \"end\": 129\n",
    "              }\n",
    "            ],\n",
    "            \"discipline\": None,\n",
    "            \"ids\": None\n",
    "          },\n",
    "          \"d56a30018f53ddb94b8f0bc778bf422fa0c3aa96\": {\n",
    "            \"bib_entry_raw\": \"Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531.\",\n",
    "            \"contained_arXiv_ids\": [],\n",
    "            \"contained_links\": [],\n",
    "            \"discipline\": None,\n",
    "            \"ids\": None\n",
    "          },\n",
    "          \"3f88da7c4bf7953e0744300aa749ad4ea51ccf64\": {\n",
    "            \"bib_entry_raw\": \"Heng Ji and Ralph Grishman. 2006. Data selection in semi-supervised learning for name tagging. In Proceedings of the Workshop on Information Extraction Beyond The Document, pages 48–55, Sydney, Australia. Association for Computational Linguistics.\",\n",
    "            \"contained_arXiv_ids\": [],\n",
    "            \"contained_links\": [\n",
    "              {\n",
    "                \"url\": \"https://www.aclweb.org/anthology/W06-0206\",\n",
    "                \"text\": \"Data selection in semi-supervised learning for name tagging. In\",\n",
    "                \"start\": 34,\n",
    "                \"end\": 97\n",
    "              }\n",
    "            ],\n",
    "            \"discipline\": None,\n",
    "            \"ids\": None\n",
    "          },\n",
    "          \"bd09f69b1c6b8444ac39821d741e6ab9c7ca505d\": {\n",
    "            \"bib_entry_raw\": \"Katrin Kirchhoff and Jeff Bilmes. 2014. Submodularity for data selection in machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 131–141.\",\n",
    "            \"contained_arXiv_ids\": [],\n",
    "            \"contained_links\": [],\n",
    "            \"discipline\": None,\n",
    "            \"ids\": None\n",
    "          },\n",
    "          \"ce1fd2a2f25b5cf04b392c438b3c6b0df2228815\": {\n",
    "            \"bib_entry_raw\": \"Jeremiah Liu, John W. Paisley, M. Kioumourtzoglou, and B. Coull. 2019a. Accurate uncertainty estimation and decomposition in ensemble learning. In NeurIPS.\",\n",
    "            \"contained_arXiv_ids\": [],\n",
    "            \"contained_links\": [],\n",
    "            \"discipline\": None,\n",
    "            \"ids\": None\n",
    "          },\n",
    "          \"9d69ca232191bab14501dcac52f2f58b9fe70b8a\": {\n",
    "            \"bib_entry_raw\": \"Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. 2019b. Improving multi-task deep neural networks via knowledge distillation for natural language understanding. arXiv preprint arXiv:1904.09482.\",\n",
    "            \"contained_arXiv_ids\": [],\n",
    "            \"contained_links\": [],\n",
    "            \"discipline\": None,\n",
    "            \"ids\": None\n",
    "          },\n",
    "          \"aac8015edb82c072cece8b4c602023d445ebf1ef\": {\n",
    "            \"bib_entry_raw\": \"David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proceedings of the main conference on human language technology conference of the North American Chapter of the Association of Computational Linguistics, pages 152–159. Association for Computational Linguistics.\",\n",
    "            \"contained_arXiv_ids\": [],\n",
    "            \"contained_links\": [],\n",
    "            \"discipline\": None,\n",
    "            \"ids\": None\n",
    "          },\n",
    "          \"6afa85c303a47aeb1fb8a9267575fea1ff031bcd\": {\n",
    "            \"bib_entry_raw\": \"Takeru Miyato, Andrew M. Dai, and Ian J. Goodfellow. 2017. Adversarial training methods for semi-supervised text classification. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.\",\n",
    "            \"contained_arXiv_ids\": [],\n",
    "            \"contained_links\": [\n",
    "              {\n",
    "                \"url\": \"https://openreview.net/forum?id=r1X3g2_xl\",\n",
    "                \"text\": \"Adversarial training methods for semi-supervised text classification. In\",\n",
    "                \"start\": 59,\n",
    "                \"end\": 131\n",
    "              }\n",
    "            ],\n",
    "            \"discipline\": None,\n",
    "            \"ids\": None\n",
    "          },\n",
    "          \"cf0cfc9674d128c71bd1466afe94cc889673cc14\": {\n",
    "            \"bib_entry_raw\": \"Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. 2019. Virtual adversarial training: A regularization method for supervised and semi-supervised learning. IEEE Trans. Pattern Anal. Mach. Intell., 41(8):1979–1993.\",\n",
    "            \"contained_arXiv_ids\": [],\n",
    "            \"contained_links\": [\n",
    "              {\n",
    "                \"url\": \"https://doi.org/10.1109/TPAMI.2018.2858821\",\n",
    "                \"text\": \"Virtual adversarial training: A regularization method for supervised and semi-supervised learning.\",\n",
    "                \"start\": 71,\n",
    "                \"end\": 169\n",
    "              }\n",
    "            ],\n",
    "            \"discipline\": None,\n",
    "            \"ids\": None\n",
    "          },\n",
    "          \"99fac70d55c1bc97b0748c221563af348f87c342\": {\n",
    "            \"bib_entry_raw\": \"Avital Oliver, Augustus Odena, Colin Raffel, Ekin D. Cubuk, and Ian J. Goodfellow. 2018. Realistic evaluation of deep semi-supervised learning algorithms.\",\n",
    "            \"contained_arXiv_ids\": [\n",
    "              {\n",
    "                \"id\": \"1804.09170\",\n",
    "                \"text\": \"Realistic evaluation of deep semi-supervised learning algorithms.\",\n",
    "                \"start\": 89,\n",
    "                \"end\": 154\n",
    "              }\n",
    "            ],\n",
    "            \"contained_links\": [],\n",
    "            \"discipline\": None,\n",
    "            \"ids\": None\n",
    "          },\n",
    "          \"4b1c521cd62705dc38bd084dc0587713b9b6d458\": {\n",
    "            \"bib_entry_raw\": \"Sree Hari Krishnan Parthasarathi and Nikko Strom. 2019. Lessons from building acoustic models with a million hours of speech. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6670–6674. IEEE.\",\n",
    "            \"contained_arXiv_ids\": [],\n",
    "            \"contained_links\": [],\n",
    "            \"discipline\": None,\n",
    "            \"ids\": None\n",
    "          },\n",
    "          \"1e2b68ee0b0825ec0bfbb1f0072097f7f02d9f80\": {\n",
    "            \"bib_entry_raw\": \"P. J. Price. 1990. Evaluation of spoken language systems: the atis domain. In HLT.\",\n",
    "            \"contained_arXiv_ids\": [],\n",
    "            \"contained_links\": [],\n",
    "            \"discipline\": None,\n",
    "            \"ids\": None\n",
    "          },\n",
    "          \"5784b04dd9b18b7e5a157a6ccd1dec31df8db12d\": {\n",
    "            \"bib_entry_raw\": \"Sebastian Ruder and Barbara Plank. 2018. Strong baselines for neural semi-supervised learning under domain shift. Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).\",\n",
    "            \"contained_arXiv_ids\": [],\n",
    "            \"contained_links\": [\n",
    "              {\n",
    "                \"url\": \"https://doi.org/10.18653/v1/p18-1096\",\n",
    "                \"text\": \"Strong baselines for neural semi-supervised learning under domain shift.\",\n",
    "                \"start\": 41,\n",
    "                \"end\": 113\n",
    "              }\n",
    "            ],\n",
    "            \"discipline\": None,\n",
    "            \"ids\": None\n",
    "          },\n",
    "          \"ffa26c06c36fb86e50a9af61f06524b3aac58a04\": {\n",
    "            \"bib_entry_raw\": \"Kai Wei, Rishabh Iyer, and Jeff Bilmes. 2015. Submodularity in data subset selection and active learning. In International Conference on Machine Learning, pages 1954–1963.\",\n",
    "            \"contained_arXiv_ids\": [],\n",
    "            \"contained_links\": [],\n",
    "            \"discipline\": None,\n",
    "            \"ids\": None\n",
    "          },\n",
    "          \"78061be39236427e5c5bb4e974c01ba81ce49c0b\": {\n",
    "            \"bib_entry_raw\": \"I Zeki Yalniz, Hervé Jégou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. 2019. Billion-scale semi-supervised learning for image classification. arXiv preprint arXiv:1905.00546.\",\n",
    "            \"contained_arXiv_ids\": [],\n",
    "            \"contained_links\": [],\n",
    "            \"discipline\": None,\n",
    "            \"ids\": None\n",
    "          },\n",
    "          \"943c6d5534f1df1ba12d04a712ad573474152d33\": {\n",
    "            \"bib_entry_raw\": \"David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In 33rd annual meeting of the association for computational linguistics, pages 189–196.\",\n",
    "            \"contained_arXiv_ids\": [],\n",
    "            \"contained_links\": [],\n",
    "            \"discipline\": None,\n",
    "            \"ids\": None\n",
    "          },\n",
    "          \"ab0caeccc5148785b654d9797a05459664d1c7ff\": {\n",
    "            \"bib_entry_raw\": \"Zhi-Hua Zhou and Ming Li. 2005. Tri-training: Exploiting unlabeled data using three classifiers. IEEE Transactions on knowledge and Data Engineering, 17(11):1529–1541.\",\n",
    "            \"contained_arXiv_ids\": [],\n",
    "            \"contained_links\": [],\n",
    "            \"discipline\": None,\n",
    "            \"ids\": None\n",
    "          }\n",
    "        },\n",
    "        \"inlined_texts\": [\n",
    "          {\n",
    "            \"section\": \"Introduction\",\n",
    "            \"text\": \"Voice-assistants with speech and natural language understanding (NLU) are becoming increasingly prevalent in every day life. These systems, such as Google Now, Alexa, or Siri, are able to respond to queries pertaining multiple domains (e.g., music, weather). An NLU system commonly consists of an intent classifier (IC) and named entity recognizer (NER). It takes text input from an automatic speech recognizer and predicts intent and entities. For example, if a user asks “play lady gaga”, the IC classifies the query to intent of PlayMusic, and the NER classifies “lady gaga” as Artist. An important requirement for voice-assistants is the ability to continuously add support for new functionalities, i.e., new intents, or new entity types, while improving recognition accuracy for the existing ones. Having high quality labeled data is the key to achieve this goal. However, obtaining human annotation is an expensive and time-consuming process.\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Introduction\",\n",
    "            \"text\": \"Semi-Supervised Learning (SSL) provides a framework for utilizing large amount of unlabeled data when obtaining labels is expensive , , . SSL techniques have been shown to improve deep models performance across different machine learning tasks including text classification, machine translation, image classification , , , , , . A common practice to evaluate SSL algorithms is to take an existing labeled dataset and only use a small fraction of training data as labeled data, while treating the rest of the data as unlabeled dataset. Such evaluation, often constrained to the cases when labeled data is scarce, raises questions about the usefulness of different SSL algorithms in a real-world setting .\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Introduction\",\n",
    "            \"text\": \"In voice assistants, we face additional challenges while applying SSL techniques at scale including (1) how much unlabeled data should we use for SSL and how to select unlabeled data from a large pool of unlabeled data? (2) Most SSL benchmarks make the assumption that unlabeled datasets come from the same distribution as the labeled datasets. This assumption is often violated as, by design, the labeled training datasets also contain synthetic data, crowd-sourced data to represent anticipated usages of a functionality, and unlabeled data often contain a lot of out of domain data. (3) Unlike widely used NLU datasets such as SNIPS , ATIS , real-world voice assistant datasets are much larger and have a lot of redundancy because some queries such as turn on lights might be much more frequent than others. Due to such evaluation concerns, performance of different SSL techniques in real-world NLU applications is still in question.\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Introduction\",\n",
    "            \"text\": \"To address these issues, we study three data selection methods to select unlabeled data and evaluate how the selected data affect the performance of different SSL methods on a real-world NLU dataset. This paper provides three contributions: (1) Design of a production SSL pipeline which can be used to intelligently select unlabeled data to train SSL models (2) Experimental comparison of four SSL techniques including, Pseudo-Label, Knowledge Distillation, Cross-View Training, and Virtual Adversarial Training in a real-world voice assistant setting (3) Operational recommendations for NLP practitioners who would like to employ SSL in production setting.\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Background\",\n",
    "            \"text\": \"Semi-Supervised Learning techniques are capable of providing large improvements in model performance with little effort, which could play a crucial role in large scale systems in industry. In supervised learning, given a labeled dataset $\\\\mathcal {D}_l$  composed of input-label pairs $(x,y)$ , the goal is to learn a prediction model $f_{\\\\theta }(x)$ , with parameters $\\\\theta$ , that is able to predict the correct label $y^{\\\\prime }$  corresponding to a new unseen input instance $x^{\\\\prime }$ . SSL techniques aim to leverage an unlabeled dataset, $\\\\mathcal {D}_u$ , to create better performing models than those that could be obtained by only using $\\\\mathcal {D}_l$ .\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Background\",\n",
    "            \"text\": \"The two widely used SSL methods are: Pseudo-Label (PL), and Knowledge Distillation (KD). In PL, a teacher model trained on labeled data is used to produce pseudo-labels for the unlabeled data set. A student model trained on the union of the labeled and pseudo-labeled data sets, often outperforms the teacher model. , . On the other hand, KD SSL methods do not assign a particular label to an unlabeled instance, but instead consider the whole distribution over the label space , , . In KD, it is hypothesized that leveraging the probability distribution over all labels provides more information than assuming a definitive label belonging to one particular class .\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Background\",\n",
    "            \"text\": \"In addition to PL and KD, Virtual Adversarial Training (VAT) and Cross-View Training (CVT) have achieved state-of-the-art SSL performance on various tasks including text classification, named entity recognition, and dependency parsing , , , . In this paper, we conduct comprehensive experiments and analysis related to these commonly used SSL techniques, and discuss their pros and cons in the industry setting.\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Background\",\n",
    "            \"text\": \"Data selection for SSL has been explored for different tasks including image classification  , NER  , . Model confidence based data selection is a widely used technique for SSL data selection where unlabeled data is selected on the basis of a classifier's confidence. Due to the abundance of unlabeled data in production voice-assistants, model confidence based filtering leads to a very large data pool. To overcome this issue, we study different data selection algorithm which can further reduce the size of unlabeled data.\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Methods\",\n",
    "            \"text\": \"We are interested in studying two different questions relevant to the use of unlabeled data in production environments: 1) how to effectively select SSL data from a large pool of unlabeled data, and 2) how do SSL techniques perform in realistic scenarios?\\nTo do so, we focus on the tasks of intent classification (IC) and named entity recognition (NER), two important components in NLU systems.\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Methods\",\n",
    "            \"text\": \"The model architecture we study is an LSTM-based multi-task model for IC and NER tasks, where we use 300-dimension fastText word embeddings , trained on a large voice assistant corpus.The text corpus contains data transcribed by an automatic speech recognition system. A shared 256-dimension Bi-LSTM encoder and two separate task-specific Bi-LSTM encoders (256-dimension) are applied to encode the sentences. A softmax layer and a conditional random field (CRF) layer are used to produce predictions for IC and NER, respectively.\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Methods\",\n",
    "            \"text\": \"Below we describe our implementation of the SSL techniques and the data selection methods studied.\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Data Selection Approaches\",\n",
    "            \"text\": \"In the industry setting, we often encounter the situation where we have extremely large pool of unlabeled data, intractable to have SSL methods run on the entire dataset. Given this challenge, we propose a two stage data selection pipeline to create an unlabeled SSL pool, $\\\\mathcal {D}_u$ , of a practical size, from the much larger pool of available data.\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Data Selection Approaches\",\n",
    "            \"text\": \"Data selection pipeline, shown in Figure REF , first uses a classifier's confidence score to filter domain specific unlabeled data from a very large pool of unlabeled data, which might contain data from multiple domains. For a production system, first stage filtering might result in millions of examples, so we further filter data using different selection algorithms to find an SSL data pool, which facilitates effective SSL training. While the first stage filtering tries to find domain specific examples from a large pool, the goal of the second stage filtering is to find a subset of data which could result in better performance in SSL training.\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Data Selection Approaches\",\n",
    "            \"text\": \"For first stage filtering, we train a binary classifier on the labelled data, and use it to select the in-domain unlabelled data. In our experiments, switching between different binary classifiers (linear, CNN, LSTM, etc) does not significantly change the selected data. Consequently, in this study, we simply use a single-layer 256-dimension Bi-LSTM for the first stage of filtering. Based on our initial experiments, we use confidence score 0.5 as the threshold for data selectionWe tried confidence larger than 0.5 but found that a high confidence score degrades the performance. Our hypothesis is that a high confidence score leads to selecting data similar to labeled data hence a less diverse SSL pool.. For second stage filtering, we explore data selection using a committee of models and using submodular optimization. While this paper explores only two data selection methods, it's worth mentioning that any data selection algorithm can be used in the second stage filtering to further optimize the size of SSL pool.\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Data Selection Approaches\",\n",
    "            \"text\": \"Selection by Submodular Optimization: Submodular data selection is used to select a diverse representative subset of samples from given dataset. This method has been applied in speech recognition , machine translation  and natural language understanding tasks . For SSL data selection, we use feature-based submodular selection , where submodular functions are given by weighted sums of non-decreasing concave functions applied to modular functions. For SSL data selection, we use 1-4 n-gram as features and logarithm as the concave function. We filter out any n-gram features which appear less than 30 times in $\\\\mathcal {D}_l \\\\cup \\\\mathcal {D}_u$ . The lazy greedy algorithm is used to optimize submodular functions. The algorithm starts with $\\\\mathcal {D}_l$  as the selected data and chooses the utterance from the candidate pool $\\\\mathcal {D}_u$  which provides maximum marginal gain.\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Data Selection Approaches\",\n",
    "            \"text\": \"Selection by Committee: SSL techniques work well when the model is able to provide an accurate prediction on unlabeled data. However, when this is not the case, SSL can have a detrimental effect to the overall system, since the model could be creating SSL data that is annotated incorrectly. Ideally, we would like to have a way of detecting when this might be the case.\\nTypically, for a given input $x$ , neural networks provide a point estimate that is interpreted as a probability distribution over labels. If the point $x$  is easy to learn, neural networks trained from different initial conditions will learn a similar probability distribution for $x$ . On the other hand, if $x$  is difficult to learn, their predictions are likely to disagree or converge to low confidence predictions. This phenomenon has been observed in several works addressing uncertainty estimation , . As a consequence, data points with high uncertainty are more likely to be incorrectly predicted than those with low uncertainty.\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Data Selection Approaches\",\n",
    "            \"text\": \"To detect data points on which the model is not reliable, we train a committee of $n$  teacher models (we use $n = 4$  in this paper), and compute the average entropy of the probability distribution for every data point. Specifically, let $P(y;x,\\\\theta _i)$  denote the probability of label $y$  for input $x$  according to the $i^{th}$  teacher, we compute the average entropy of the predicted label distribution of $x$  as: $H(x) = - \\\\frac{1}{n} \\\\sum _{y \\\\in \\\\mathcal {Y}} \\\\sum _{i=1}^n P(y;x,\\\\theta _i) \\\\log P(y;x,\\\\theta _i)$ .\\nWe then identify an entropy threshold with an acceptable error rate for mis-annotations (e.g., $20\\\\%$ ) based on a held-out dataset. Any committee annotated data whose entropy level is higher than the identified threshold, is deemed “not trustworthy” and filtered out.\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Semi-Supervised Learning Approaches\",\n",
    "            \"text\": \"We explore the following four Semi-Supervised Learning techniques:\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Semi-Supervised Learning Approaches\",\n",
    "            \"text\": \"PL based self-training is a simple and straightforward method of SSL , . Using a labeled data set $\\\\mathcal {D}_l$ , we first train a “teacher” model, $f_\\\\theta$ . We then generate a dataset of pseudo-labeled data from $\\\\mathcal {D}_u$ , by assigning for each input instance $x_u$ , the label $\\\\hat{y}$ , predicted by the teacher. A new model, to which we refer as a “student”, is then trained on the union of both pseudo-labeled and labeled datasets.\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Semi-Supervised Learning Approaches\",\n",
    "            \"text\": \"In KD, for a given input, a teacher model produces a probability distribution over all possible labels. The predicted probability distribution is often referred to as “soft label”. The student model is then trained alternating between two objectives: minimizing the loss on the labeled data, defined respectively for different tasks, and minimizing the cross-entropy loss between the student and teacher predicted “soft label” on the unlabeled data  . The soft labels on intents are generated by the IC's softmax layer, while the soft labels on label sequences are generated per token, by running softmax on the logits for each token before the CRF layer.\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Semi-Supervised Learning Approaches\",\n",
    "            \"text\": \"VAT is an efficient SSL approach based on adversarial learning. It has been shown to be highly effective in both image  and text classification  tasks. Given an unlabeled instance, VAT generates a small perturbation that would lead to the largest shift on the label distribution predicted by the model. After getting the adversarial perturbation, the objective is to minimize the KL divergence between the label distribution on the original instance and the instance with perturbation.\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Semi-Supervised Learning Approaches\",\n",
    "            \"text\": \"CVT is another SSL approach proved to be efficient on text classification, sequence labeling and machine translation . Using an Bi-LSTM, CVT uses the the bi-directional output from current state as an auxiliary prediction, takes the single-directional output from current and neighboring LSTM neurons, and forces them to predict the same label as the auxiliary prediction.\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Data Sets\",\n",
    "            \"text\": \"The main motivation of our study is to evaluate different data selection and SSL techniques in a production scale setting where we have a large amount of unlabeled data. To understand impact of data selection, we create two benchmark datasets for our experiments. In both experiments, using the pipeline shown in Figure REF , we first select $M$  utterances from a very large pool of unlabeled data, and then apply intelligent data selection to further select $N$  unlabeled utterances.\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Data Sets\",\n",
    "            \"text\": \"Commercial Dataset: Our commercial dataset provides an experimental setup to compare SSL techniques where labeled training data and unlabeled data come from a similar distribution. We choose four representative domains (i.e., categories for which the user can make requests) from a commercially available voice-assistant system for English language. The four selected categories are 1) Communication: queries related to call, messages, 2) Music: queries related to playing music, 3) Notifications: queries related to alarms, timers, and 4) ToDos: queries related to task organization. For each domain, NLU task is to identify the intent (IC), and the entities (NER) in the utterance.\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Data Sets\",\n",
    "            \"text\": \"For each domain, our dataset contains 50k unique training, 50k unique testing utterances, and hundreds of millions of utterances of unlabeled data. Since, we do not know in advance to which domain each unlabeled utterance belongs, we first select 500K unlabeled utterance per domain to form their respective unlabeled data pool, using a domain classifier, as shown in Figure REF . The choice of 500K size is based on a series of KD based SSL experiments in Music domain, with the SSL data pool size varying from 50K to 1M. It is observed that increasing SSL pool size beyond 500k starts to reduce the performance gain from SSL (Table REF ). To evaluate the effect of intelligent data selection, out of 500k, we further select 300k utterances via different data selection approaches and use them as unlabeled data in SSL experiments.\\n<table> Relative error rate reduction using KD, over baseline trained with only labeled data, for Music domain. Unlabeled data SSL pool size varies from 50K to 1M utterances. 50K labeled examples are used for all experiments. The metric for IC is classification error rate, and for NER is entity recognition F1 error rate.\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Data Sets\",\n",
    "            \"text\": \"SNIPS Dataset: We also create a benchmark setup where labeled and unlabeled data come from different distributions. We use SNIPS  dataset as labeled data, and use unlabeled data from our commercial dataset as SSL pool data. Similar to our commercial dataset, we train a binary classifier for each intent on SNIPS and use it to select $300,000$  utterances as the unlabeled data pool for each intent. Then, we apply data selection approaches to filter for $20,000$  utterances per intent for SSL experiments.\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Results\",\n",
    "            \"text\": \"This section presents evaluations of different SSL techniques using different data selection regimes.\\nFor all experiments, hyperparameters are optimized on development set. The SSL techniques evaluated are: PL, KD, VAT, CVT. The data selection methods evaluated are: random selection (Random), submodular optimization based selection (Submodular), and committee-based selection (Committee).\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Results on Commercial Dataset\",\n",
    "            \"text\": \"Due to confidentiality, we could not disclose absolute performance numbers on the commercial dataset. Only relative changes over baseline are reported. A summary of the results for the various data selection and SSL techniques is given in Table REF . “Baseline” refers to model trained with only labeled data. The metric for IC task is intent classification error rate. The metric for NER task is entity recognition F1 error rate. The table shows the relative error reduction compared to baseline. The bold font shows the best performing SSL method for each data selection approach.\\n<table> Model performance by different SSL methods and data selection methods, for SNIPS data set. The metric for IC task is classification error rate, and for NER task is entity recognition F1 error rate.\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Results on Commercial Dataset\",\n",
    "            \"text\": \"Comparison of Data Selection Methods: We observe that both Submodular and Committee based selection outperforms random selection across all domains and SSL techniques. This shows the effectiveness of Stage 2 data filtering. While on Notifications and ToDos domain, submodular selection performs better than other methods, on Communication and Music domain, committee based selection performs the best.\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Results on Commercial Dataset\",\n",
    "            \"text\": \"Comparison of SSL Techniques: Table REF  shows that KD improves performances over PL in virtually all scenarios (except for NER in ToDos). This supports the hypothesis that using the full distribution predicted by the teacher model, instead of using solely the predicted label, allows for the transfer of extra information when training a student model. In addition, though both VAT and CVT consistently outperform KD and PL, their benefits are task dependent. VAT shows stronger benefits on all NER experiments, while CVT performs better in most IC experiments. From an accuracy perspective, VAT is more beneficial in NER tasks while CVT is more beneficial in classification tasks.\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Results on Commercial Dataset\",\n",
    "            \"text\": \"SSL Techniques Computation Comparison: We time each SSL technique on the data selected for Music domain. While PL and KD took approximately 30 minutes to train each epoch on a Tesla V100 GPU, VAT and CVT took 62 minutes and 75 minutes, respectively. Given that PL and KD have similar compute requirement and KD consistently outperforms PL, KD should be preferred over PL for SSL. The decision between CVT and VAT relies on the trade-off between accuracy and cost.\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Results on SNIPS Dataset\",\n",
    "            \"text\": \"Test results on SNIPS dataset are summarized in Table REF . The test results on SNIPS aligns with our observations on commercial dataset: VAT and CVT are the superior SSL techniques. Moreover, the results show that VAT and CVT provide good generalization even when the labeled and unlabeled data are from different sources and of different distributions. In contrast to the commercial dataset where intelligent data selection leads to better performance, on SNIPS dataset, we found that submodular optimization or committee based selection do not provide any gain over random selection. It's not surprising given that SNIPS labeled data distribution is very different from the unlabeled SSL data which makes data selection algorithm susceptible to noisy unlabeled data selection. For example, submodular optimization primarily optimizes for data diversity which makes it more likely to select diverse unrelated examples than random selection.\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Recommendations\",\n",
    "            \"text\": \"Based on our empirical results, we make the following recommendations for industry scale NLU SSL systems.\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Recommendations\",\n",
    "            \"text\": \"Prefer VAT and CVT SSL techniques over PL and KL: When selecting SSL techniques, CVT usually performs better for classification task while VAT is preferable for NER task. In general, we would recommend VAT since its performance in classification task is comparable to CVT and also because VAT excels in NER task which is usually harder to achieve performance gain.\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Recommendations\",\n",
    "            \"text\": \"Use data selection to select a subset of unlabeled data: For industry setting where the volume of unlabeled data is impractically large, we introduce a data filtering pipeline to first reduce the size of unlabeled data pool to a manageable size. Our experiments show that both submodular as well as committee based data selection could further improve SSL performance. We recommend Submodular Optimization based data selection in light of its lower cost and similar performance to committee based method.\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Recommendations\",\n",
    "            \"text\": \"From experiments on SNIPS data sets, we observe that further data selection does not bring extra improvement comparing to random selection. Optimizing data selection, when unlabeled data pool is of a drastically different distribution from the labeled data, remains a challenge and could benefit from further research.\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Conclusion\",\n",
    "            \"text\": \"In this paper, we conduct extensive experiments and in-depth analysis of different SSL techniques applied to industry scale NLU tasks. Industrial settings come with some unique challenges such as massive unlabeled data with a mixture of in domain and out of domain data. In order to overcome these challenges, we also investigate different data selection approaches including submodular optimization and committee based filtering.\\n\"\n",
    "          },\n",
    "          {\n",
    "            \"section\": \"Conclusion\",\n",
    "            \"text\": \"Our paper provides insights on how to build an efficient and accurate NLU system, utilizing SSL, from different perspectives (e.g. model accuracy, amount of data, training time and cost, etc). By sharing these insights with larger NLP community, we hope that these guideline will be useful for researchers and practitioner who aim to improve NLU systems while minimizing human annotation effort.\\n\"\n",
    "          }\n",
    "        ]\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"\\\n",
    "Your task is to index machine learning research papers for a knowledge base. Given Markdown text of a research paper, your goal is to extract all relevant information and return it in a structured format.\n",
    "\n",
    "Read the following instructions and return your response as a JSON object of type `Response` inside a Markdown code block.\n",
    "\n",
    "```ts\n",
    "type Response = {\n",
    "    // Extract all findings, results, and insights from the paper. Remember, assume a relatively basic level of background knowledge, so be thorough in picking up information. Required.\n",
    "    findings: Finding[];\n",
    "    // List all tasks discussed in the paper. Required.\n",
    "    tasks: Task[];\n",
    "    // List all benchmarks discussed in the paper. Required.\n",
    "    benchmarks: Benchmark[];\n",
    "    // List all architectures discussed in the paper. Required.\n",
    "    architectures: Architecture[];\n",
    "    // List all models discussed in the paper. Required.\n",
    "    models: Model[];\n",
    "    // List all methods discussed in the paper. Required.\n",
    "    methods: Method[];\n",
    "    // List all datasets discussed in the paper. Required.\n",
    "    datasets: Dataset[];\n",
    "};\n",
    "\n",
    "// List all informative findings from the paper because the original will be discarded later. Required.\n",
    "// Every finding must be named to at least one topic later. Make sure you mention these topics by name so the connections are apparent.\n",
    "type Finding = {\n",
    "    // Unique identifier for the Finding. Use only lowercase alphanumeric characters and underscores. Required.\n",
    "    slug: string;\n",
    "    // Short name for the finding that could be used to retrieve it in a search engine. Required.\n",
    "    name: string;\n",
    "    // A clear, accessible, and impartial summary of the finding. Use at most five sentences. The description should make sense when read alone. Required.\n",
    "    description: string;\n",
    "};\n",
    "\n",
    "// A topic is a concept or idea that is discussed in the paper and linked to findings. Do not merge similar topics into one.\n",
    "type BaseTopic = {\n",
    "    // Unique identifier for the Topic. Use only lowercase alphanumeric characters and underscores. Required.\n",
    "    slug: string;\n",
    "    // Short name for the topic that could be used to retrieve it in a search engine. Required.\n",
    "    name: string;\n",
    "    // A clear, accessible, and impartial summary of the topic. Use at most five sentences. The description should make sense when read out of context. Required.\n",
    "    description: string;\n",
    "    // Findings related to the topic. Reference these by their slugs. Must not be empty. Required.\n",
    "    linked_findings: string[];\n",
    "};\n",
    "\n",
    "// A task is a specific problem class. For example, \"sentiment analysis\" is a task. Other tasks may include \"named entity recognition\" or \"GPU utilization\". Do not merge similar tasks into one.\n",
    "// Each task must be linked to at least one finding.\n",
    "type Task = BaseTopic & {\n",
    "    type: \"task\";\n",
    "};\n",
    "\n",
    "// A benchmark is a standardized evaluation suite that evaluates the performance of a model on a specific task. Only create benchmarks if the name and description are clear. Do not count metrics such as BLEU as benchmarks. Do not merge similar benchmarks into one.\n",
    "// Each benchmark must be linked to at least one finding.\n",
    "type Benchmark = BaseTopic & {\n",
    "    type: \"benchmark\";\n",
    "};\n",
    "\n",
    "// An architecture is a specific type of model. For example, \"LSTM\" is an architecture. Do not merge similar architectures into one.\n",
    "// Each architecture must be linked to at least one finding.\n",
    "type Architecture = BaseTopic & {\n",
    "    type: \"architecture\";\n",
    "};\n",
    "\n",
    "// A specific instance of a model listed in the paper. For example, \"BERT\" is a model. General architectures like \"transformer\" should not be listed as models. Do not merge similar models into one.\n",
    "// Each model must be linked to at least one finding.\n",
    "type Model = BaseTopic & {\n",
    "    type: \"model\";\n",
    "};\n",
    "\n",
    "// A method is a technique or algorithm. Metrics should be counted as Methods instead of Benchmarks. For example, a \"convolutional neural network\" is a type of method. Do not merge similar methods into one.\n",
    "// Each method must be linked to at least one finding.\n",
    "type Method = BaseTopic & {\n",
    "    type: \"method\";\n",
    "};\n",
    "\n",
    "// A dataset is a collection of data used to train or evaluate a model. For example, \"MNIST\" is a dataset.\n",
    "// Each dataset must be linked to at least one finding. Only create datasets if the name and description are clear. Do not merge similar datasets into one.\n",
    "type Dataset = BaseTopic & {\n",
    "    type: \"dataset\";\n",
    "};\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "current_section = None\n",
    "paper_text = \"\"\n",
    "for paragraph in paper[\"inlined_texts\"]:\n",
    "    # paper_text += paragraph.text + \"\\n\"\n",
    "    if paragraph[\"section\"] and paragraph[\"section\"].strip() != current_section:\n",
    "        para = paragraph[\"section\"].strip()\n",
    "        paper_text += f\"## {para}\\n\"\n",
    "        current_section = para\n",
    "\n",
    "    paper_text += paragraph[\"text\"].strip() + \"\\n\"\n",
    "\n",
    "\n",
    "query = f\"\"\"\n",
    "user input:\n",
    "\n",
    "```markdown\n",
    "# \"Industry Scale Semi-Supervised Learning for Natural Language Understanding\"\n",
    "\n",
    "## Abstract\n",
    "\"This paper presents a production Semi-Supervised Learning (SSL) pipeline\\nbased on the student-teacher framework, which leverages millions of unlabeled\\nexamples to improve Natural Language Understanding (NLU) tasks. We investigate\\ntwo questions related to the use of unlabeled data in production SSL context:\\n1) how to select samples from a huge unlabeled data pool that are beneficial\\nfor SSL training, and 2) how do the selected data affect the performance of\\ndifferent state-of-the-art SSL techniques. We compare four widely used SSL\\ntechniques, Pseudo-Label (PL), Knowledge Distillation (KD), Virtual Adversarial\\nTraining (VAT) and Cross-View Training (CVT) in conjunction with two data\\nselection methods including committee-based selection and submodular\\noptimization based selection. We further examine the benefits and drawbacks of\\nthese techniques when applied to intent classification (IC) and named entity\\nrecognition (NER) tasks, and provide guidelines specifying when each of these\\nmethods might be beneficial to improve large scale NLU systems.\\n\"\n",
    "\n",
    "{paper_text[:16_000]}\n",
    "```\n",
    "When extracting information, assume a relatively basic level of background knowledge. Your response should be concise and informative, focusing on the key aspects of the paper. Keep your JSON concise by omitting missing properties instead of explicitly setting them to `null`, `undefined`, or an empty string/array. Do not indent your JSON response.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.apply_chat_template(\n",
    "      [\n",
    "            {\n",
    "                  \"role\": \"user\",\n",
    "                  \"content\": query,\n",
    "            },\n",
    "      ],\n",
    "      tokenize=False,\n",
    "      add_generation_prompt=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine_tune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
